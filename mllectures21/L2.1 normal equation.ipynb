{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "In statistics, the objective of Regression Analysis is to estimate the relationship among variables. Using Linear Regression, we can describe a relationship between variables with a linear function. We can use such a function to estimate the target variable for one or more input variables. Often, the target variable takes on a real number as its value.\n",
    "\n",
    "By applying regression, we hope to find some function `f(x)` that for any given input variable $x$ predicts our target variable $y$. We call this the **target function** or the **true function**:\n",
    "\n",
    "$$\n",
    "f(x) = y\n",
    "$$\n",
    "\n",
    "# Hypothesis\n",
    "\n",
    "A **hypothesis** is a function that we hope (hypothesize) is similar to the true function. So for the case of predicting the quality of wine, a function that takes some input variables about a wine and then correctly predicts the quality of that wine. In the case of linear regression over a single input variable $x$, we hope that we can find some linear function that correctly predicts our target variable $y$. The estimated target variable is written as $\\widehat{y}$. For a linear function, we need to fit two **parameters**: the intercept (the y-value as $x=0$) and the slope (the extent to which $y$ changes with $x$). For convenience, we store those two parameters in a vector $theta$: $\\theta_0$ is the intercept and $\\theta_1$ is the slope. We say that the hypothesis $h$ is parameterized by the coefficients in $\\theta$ and therefore write $h_\\theta$ (pronounced as 'h over $\\theta$'):\n",
    "\n",
    "$$ \n",
    "h_\\theta(x) = \\widehat{y} = \\theta_1 * x + \\theta_0\n",
    "$$\n",
    "\n",
    "# Cost function\n",
    "\n",
    "Our hypothesis consists of two parts, the shape of the function (in this case a first degree equation) and its parameters. If we have chosen the shape, our next task is to find the parameters that are most optimal for the problem. For linear regression, we commonly use the **sum of least squares** to indicate how good the parameters fit the data. More formally, we define a cost function $J(\\theta)$ that indicates how good the parameters fit the data. For linear regression, we simply sum the squared difference between the predicted value and the true value for every training sample. To distinguish between samples in de dataset, we write $x^{(i)}$ and $y^{(i)}$ to reference the training sample at row $i$.\n",
    "\n",
    "$$ J(\\theta) = \\sum_i \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) ^2  = \\sum_i \\left( \\theta_1 * x^{(i)} + \\theta_0 - y^{(i)} \\right)^2 $$\n",
    "\n",
    "# Vector notation\n",
    "\n",
    "In Machine Learning, we often write our equations in vector notation. Vector notations are shorter, but we will also see that we can write our programming code in vector notation. Using the same language means that we are less likely to make mistakes in translating our learning objective to code, and as an added bonus the code is often a lot faster. \n",
    "\n",
    "We can **vectorize** our learning objective by rewriting our hypothesis as a vector product. Because we have two parameters in $\\theta$, we also need two values in $x$ so that their product equals the original hypothesis. We do this by adding a **bias** value of 1 to every $x$, so that we get:\n",
    "\n",
    "$$ x = \\left[ \\begin{matrix} x_0 = bias = 1 \\\\ x_1 = feature \\end{matrix} \\right], \\theta = \\left[ \\begin{matrix} \\theta_0 = \\text{intercept} \\\\ \\theta_1 = slope \\end{matrix} \\right], \\text{having}\\ x_0 = 1\n",
    "$$\n",
    "\n",
    "We can then rewrite the hypothesis as a vector product (for newcomers to vector notations: $\\theta^T$ is the *transpose* of $\\theta$, which in this case is a row vector of $\\theta$):\n",
    "\n",
    "$$h_\\theta(x) = \\theta^T \\cdot x = \\theta_0 * x_0 + \\theta_1 * x_1$$\n",
    "\n",
    "Using vector notation, our cost function becomes:\n",
    "\n",
    "$$ J(\\theta) = \\sum_i \\left( \\theta^T \\cdot x^{(i)} - y^{(i)} \\right)^2 $$\n",
    "\n",
    "# Data matrix\n",
    "\n",
    "The next step in vectoriation is to place our training samples in a **Data matrix**. We define a matrix $X$ in which the input samples $(x^{(i)})$ are placed as row vectors $(x^{(i)})^T$:\n",
    "\n",
    "$$ X = \\left[ \\begin{matrix} \n",
    "x_0^{(0)} & x_1^{(0)} \\\\\n",
    "x_0^{(1)} & x_1^{(1)} \\\\\n",
    "\\dots & \\dots \\\\\n",
    "x_0^{(m-1)} & x_1^{(m-1)}\n",
    "\\end{matrix} \\right]\n",
    "$$\n",
    "\n",
    "Since $y$ is a column vector of the target variable, we can rewrite the cost function as a matrix multiplication. To make the transformation complete, we also replace the square by a matrix dot product:\n",
    "\n",
    "$$ J(\\theta) = \\sum_i \\left( \\theta^T \\cdot x_i - y^{(i)} \\right)^2 = (X \\cdot \\theta - y)^T \\cdot (X \\cdot \\theta - y) $$ \n",
    "\n",
    "# Normal Equation\n",
    "\n",
    "Now that we have rewritten our cost function into vector notation, we can analytically determine for which values of $\\theta$ the cost function is minimal. Beacuse the cost function is a squared or second order function, we know that it must looks like a parabole or bowl shape. The minimum is where the derivative equals zero:\n",
    "\n",
    "$$ \\frac{\\delta J(\\theta)}{\\delta \\theta} = 2X^TX\\theta - 2X^Ty = 0 $$\n",
    "\n",
    "If you solve this linear algebra equation you get the so-called Normal Equation that gives you the optimal $\\theta$ for the cost function:\n",
    "\n",
    "$$ \\theta = (X^TX)^{-1} \\cdot X^Ty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data loader in the ML library already sets up the input in a data matrix $X$ and a target vector $y$. The predefined models (e.g. linear_regression_ne, knn) already add a bias column, so we do not need to add that. And the underlying implementation wants the target vector $y$ to be a row vector instead of a column vector. To setup the data with a bias column and the column vector for the target variable we add `bias=True` and `column_y=True`. \n",
    "\n",
    "Note: we only need to transform the data for implementing the mathematical derivation, do not use this when using ML or SKLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data, but include a bias parameter\n",
    "data = advertising_sales_tv(bias=True, column_y=True)\n",
    "X = data.train_X\n",
    "y = data.train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1. ,  93.9],\n",
       "       [  1. ,  75.1],\n",
       "       [  1. ,   4.1],\n",
       "       [  1. , 195.4],\n",
       "       [  1. , 261.3],\n",
       "       [  1. , 276.9],\n",
       "       [  1. , 141.3],\n",
       "       [  1. ,   0.7],\n",
       "       [  1. , 228.3],\n",
       "       [  1. , 171.3],\n",
       "       [  1. , 112.9],\n",
       "       [  1. , 187.9],\n",
       "       [  1. , 109.8],\n",
       "       [  1. ,   8.4],\n",
       "       [  1. , 255.4],\n",
       "       [  1. ,   7.8],\n",
       "       [  1. , 281.4],\n",
       "       [  1. , 292.9],\n",
       "       [  1. , 276.7],\n",
       "       [  1. , 188.4],\n",
       "       [  1. , 120.5],\n",
       "       [  1. , 129.4],\n",
       "       [  1. , 109.8],\n",
       "       [  1. ,   5.4],\n",
       "       [  1. , 293.6],\n",
       "       [  1. , 219.8],\n",
       "       [  1. ,  17.2],\n",
       "       [  1. ,  97.5],\n",
       "       [  1. , 240.1],\n",
       "       [  1. , 213.4],\n",
       "       [  1. ,   8.7],\n",
       "       [  1. ,  78.2],\n",
       "       [  1. , 280.2],\n",
       "       [  1. , 218.5],\n",
       "       [  1. ,  18.8],\n",
       "       [  1. , 215.4],\n",
       "       [  1. , 164.5],\n",
       "       [  1. ,  62.3],\n",
       "       [  1. ,  96.2],\n",
       "       [  1. , 217.7],\n",
       "       [  1. ,   8.6],\n",
       "       [  1. , 182.6],\n",
       "       [  1. , 240.1],\n",
       "       [  1. , 137.9],\n",
       "       [  1. , 125.7],\n",
       "       [  1. , 163.5],\n",
       "       [  1. , 206.9],\n",
       "       [  1. , 136.2],\n",
       "       [  1. , 234.5],\n",
       "       [  1. ,  13.2],\n",
       "       [  1. , 156.6],\n",
       "       [  1. , 191.1],\n",
       "       [  1. , 172.5],\n",
       "       [  1. , 110.7],\n",
       "       [  1. ,  36.9],\n",
       "       [  1. , 102.7],\n",
       "       [  1. ,  73.4],\n",
       "       [  1. , 166.8],\n",
       "       [  1. ,  48.3],\n",
       "       [  1. , 175.1],\n",
       "       [  1. , 290.7],\n",
       "       [  1. ,  69. ],\n",
       "       [  1. , 199.8],\n",
       "       [  1. ,  87.2],\n",
       "       [  1. , 289.7],\n",
       "       [  1. ,  67.8],\n",
       "       [  1. , 147.3],\n",
       "       [  1. ,  13.1],\n",
       "       [  1. ,  25.1],\n",
       "       [  1. , 237.4],\n",
       "       [  1. ,  27.5],\n",
       "       [  1. , 193.7],\n",
       "       [  1. , 175.7],\n",
       "       [  1. ,  66.1],\n",
       "       [  1. , 213.5],\n",
       "       [  1. , 214.7],\n",
       "       [  1. , 198.9],\n",
       "       [  1. ,  88.3],\n",
       "       [  1. , 248.4],\n",
       "       [  1. , 241.7],\n",
       "       [  1. ,  25.6],\n",
       "       [  1. , 199.8],\n",
       "       [  1. , 273.7],\n",
       "       [  1. ,  97.2],\n",
       "       [  1. ,  70.6],\n",
       "       [  1. , 228. ],\n",
       "       [  1. , 205. ],\n",
       "       [  1. ,   7.3],\n",
       "       [  1. , 139.5],\n",
       "       [  1. , 149.7],\n",
       "       [  1. ,  28.6],\n",
       "       [  1. , 210.7],\n",
       "       [  1. , 222.4],\n",
       "       [  1. , 266.9],\n",
       "       [  1. , 170.2],\n",
       "       [  1. , 216.4],\n",
       "       [  1. ,  75.5],\n",
       "       [  1. , 227.2],\n",
       "       [  1. , 197.6],\n",
       "       [  1. ,  16.9],\n",
       "       [  1. , 121. ],\n",
       "       [  1. ,  76.4],\n",
       "       [  1. , 135.2],\n",
       "       [  1. , 229.5],\n",
       "       [  1. , 187.8],\n",
       "       [  1. ,  76.4],\n",
       "       [  1. , 193.2],\n",
       "       [  1. ,  44.5],\n",
       "       [  1. , 209.6],\n",
       "       [  1. , 117.2],\n",
       "       [  1. , 139.2],\n",
       "       [  1. ,  69.2],\n",
       "       [  1. ,  38.2],\n",
       "       [  1. , 262.7],\n",
       "       [  1. , 286. ],\n",
       "       [  1. , 248.8],\n",
       "       [  1. ,  80.2],\n",
       "       [  1. ,  18.7],\n",
       "       [  1. ,  66.9],\n",
       "       [  1. , 107.4],\n",
       "       [  1. , 218.4],\n",
       "       [  1. ,  74.7],\n",
       "       [  1. , 116. ],\n",
       "       [  1. , 123.1],\n",
       "       [  1. , 265.6],\n",
       "       [  1. , 120.2],\n",
       "       [  1. , 239.3],\n",
       "       [  1. , 283.6],\n",
       "       [  1. , 232.1],\n",
       "       [  1. , 149.8],\n",
       "       [  1. , 184.9],\n",
       "       [  1. , 225.8],\n",
       "       [  1. , 238.2],\n",
       "       [  1. , 216.8],\n",
       "       [  1. , 134.3],\n",
       "       [  1. ,  59.6],\n",
       "       [  1. ,  53.5],\n",
       "       [  1. ,  17.2],\n",
       "       [  1. ,  31.5],\n",
       "       [  1. , 280.7],\n",
       "       [  1. , 239.8],\n",
       "       [  1. , 142.9],\n",
       "       [  1. , 220.5],\n",
       "       [  1. , 206.8],\n",
       "       [  1. , 250.9],\n",
       "       [  1. ,  19.6],\n",
       "       [  1. ,  38. ],\n",
       "       [  1. ,  17.9],\n",
       "       [  1. ,  19.4],\n",
       "       [  1. ,  44.7],\n",
       "       [  1. ,  43. ],\n",
       "       [  1. , 284.3],\n",
       "       [  1. ,  90.4],\n",
       "       [  1. , 243.2],\n",
       "       [  1. , 237.4],\n",
       "       [  1. , 230.1],\n",
       "       [  1. , 253.8],\n",
       "       [  1. , 265.2],\n",
       "       [  1. , 197.6],\n",
       "       [  1. ,  25. ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.3],\n",
       "       [12.6],\n",
       "       [ 3.2],\n",
       "       [22.4],\n",
       "       [24.2],\n",
       "       [27. ],\n",
       "       [15.5],\n",
       "       [ 1.6],\n",
       "       [20.5],\n",
       "       [16. ],\n",
       "       [11.9],\n",
       "       [19.7],\n",
       "       [16.7],\n",
       "       [ 5.7],\n",
       "       [19.8],\n",
       "       [ 6.6],\n",
       "       [24.4],\n",
       "       [21.4],\n",
       "       [16.8],\n",
       "       [19.9],\n",
       "       [14.2],\n",
       "       [11. ],\n",
       "       [12.4],\n",
       "       [ 5.3],\n",
       "       [20.7],\n",
       "       [19.6],\n",
       "       [ 5.9],\n",
       "       [13.7],\n",
       "       [18.2],\n",
       "       [17. ],\n",
       "       [ 7.2],\n",
       "       [14.6],\n",
       "       [19.8],\n",
       "       [17.2],\n",
       "       [ 7. ],\n",
       "       [17.1],\n",
       "       [17.5],\n",
       "       [ 9.7],\n",
       "       [12.3],\n",
       "       [19.4],\n",
       "       [ 4.8],\n",
       "       [21.2],\n",
       "       [20.9],\n",
       "       [15. ],\n",
       "       [15.9],\n",
       "       [18. ],\n",
       "       [17.9],\n",
       "       [13.2],\n",
       "       [16.9],\n",
       "       [ 5.6],\n",
       "       [15.5],\n",
       "       [17.3],\n",
       "       [16.4],\n",
       "       [16. ],\n",
       "       [10.8],\n",
       "       [14. ],\n",
       "       [10.9],\n",
       "       [19.6],\n",
       "       [11.6],\n",
       "       [16.1],\n",
       "       [17.8],\n",
       "       [11.3],\n",
       "       [16.4],\n",
       "       [10.6],\n",
       "       [25.4],\n",
       "       [12.5],\n",
       "       [14.6],\n",
       "       [ 5.3],\n",
       "       [ 8.5],\n",
       "       [18.9],\n",
       "       [ 6.9],\n",
       "       [19.2],\n",
       "       [17.1],\n",
       "       [12.6],\n",
       "       [21.7],\n",
       "       [17.4],\n",
       "       [23.7],\n",
       "       [12.9],\n",
       "       [20.2],\n",
       "       [21.8],\n",
       "       [ 9.5],\n",
       "       [15.6],\n",
       "       [20.8],\n",
       "       [13.2],\n",
       "       [10.5],\n",
       "       [21.5],\n",
       "       [22.6],\n",
       "       [ 5.5],\n",
       "       [10.3],\n",
       "       [17.3],\n",
       "       [ 7.3],\n",
       "       [18.4],\n",
       "       [16.5],\n",
       "       [25.4],\n",
       "       [16.7],\n",
       "       [22.6],\n",
       "       [11.9],\n",
       "       [19.8],\n",
       "       [16.7],\n",
       "       [ 8.7],\n",
       "       [11.6],\n",
       "       [11.8],\n",
       "       [17.2],\n",
       "       [19.7],\n",
       "       [20.6],\n",
       "       [ 9.4],\n",
       "       [20.2],\n",
       "       [10.4],\n",
       "       [20.9],\n",
       "       [11.9],\n",
       "       [12.2],\n",
       "       [11.3],\n",
       "       [ 7.6],\n",
       "       [20.2],\n",
       "       [20.9],\n",
       "       [18.9],\n",
       "       [11.9],\n",
       "       [ 6.7],\n",
       "       [ 9.7],\n",
       "       [11.5],\n",
       "       [18. ],\n",
       "       [14.7],\n",
       "       [11. ],\n",
       "       [15.2],\n",
       "       [17.4],\n",
       "       [13.2],\n",
       "       [20.7],\n",
       "       [25.5],\n",
       "       [18.4],\n",
       "       [10.1],\n",
       "       [20.5],\n",
       "       [18.4],\n",
       "       [20.7],\n",
       "       [22.3],\n",
       "       [14. ],\n",
       "       [ 9.7],\n",
       "       [ 8.1],\n",
       "       [12. ],\n",
       "       [11. ],\n",
       "       [16.1],\n",
       "       [17.3],\n",
       "       [15. ],\n",
       "       [20.1],\n",
       "       [17.2],\n",
       "       [22.2],\n",
       "       [ 7.6],\n",
       "       [10.9],\n",
       "       [ 8. ],\n",
       "       [ 6.6],\n",
       "       [10.1],\n",
       "       [ 9.6],\n",
       "       [20. ],\n",
       "       [12. ],\n",
       "       [25.4],\n",
       "       [17.5],\n",
       "       [22.1],\n",
       "       [17.6],\n",
       "       [17.7],\n",
       "       [16.6],\n",
       "       [ 7.2]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equation\n",
    "\n",
    "Now that we have rewritten our cost function into vector notation, we can analytically determine for which values of $\\theta$ the cost function is minimal. Beacuse the cost function is a squared or second order function, we know that it must looks like a parabole or bowl shape. The minimum is where the derivative equals zero:\n",
    "\n",
    "$$ \\frac{\\delta J(\\theta)}{\\delta \\theta} = 2X^TX\\theta - 2X^Ty = 0 $$\n",
    "\n",
    "If you solve this linear algebra equation you get the so-called Normal Equation that gives you the optimal $\\theta$ for the cost function:\n",
    "\n",
    "$$ \\theta = (X^TX)^{-1} \\cdot X^Ty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model & Training\n",
    "\n",
    "Now that we have prepared our data in $X$ and $y$, we can use the Normal Equation to compute $\\theta$. \n",
    "\n",
    "In Python, the **Numpy** library is the standard to store the vectors and matrices and to efficiently perform linear algebra operations on them. Some of the operators are slightly different from what you are used, the `@` operator performs a dot product between matrices and the `.T` transposes a matrix.\n",
    "\n",
    "We also need to compute the inverse of $X^TX$, which we can do with numpy's pseudo inverse `np.linalg.pinv()`. It is recommended to use the pseudo-inverse because this give a numerical more stable answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝜃 = np.linalg.pinv( X.T @ X ) @ X.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.93850405],\n",
       "       [0.05524176]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "𝜃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps at the beginning of this, you would not have thought learning a model would be this easy, but sometimes applying math does make for a very elegant solution (see: the unreasonable effectiveness of mathematics (Wigner, 1960)).\n",
    "\n",
    "So now we have learned $\\theta$, we know what the parameters to our regression line $\\widehat{quality} = \\theta_1 \\cdot alcohol + \\theta_0$ are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "Now that we have estimated our model $\\theta$, we can use it to make predictions. For a single data point, we can compute $\\theta^T \\cdot x$ and for multiple data points we can add them to a data matrix $X$ and compute $X \\cdot \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the result we can use the pyplot module in the Matplotlib library (imported as `plt`. In matplotlib, `plot` is the general function that draws a figure. By passing '.' as the third parameter we change the mode from the default line-graph to a scatter-graph. \n",
    "\n",
    "To plot a regression line, we compute two points. Since the min/max feature values are resp. 0 and 300, we construct a data matrix with those two points and added a bias=1 to them. Then we can simply multiply the data matrix with $\\theta$ and we get predicted Sales for those two points in `line_y`. We then use Matplotlib to draw a line over the Scatter plot. In that code, `line_X[:, 1]` is the Numpy way to select all rows (indicated by only the alcohol percentages; all rows (indicated by the :) and just the column indexed by 1 (so it leaves out the bias).\n",
    "\n",
    "Always add labels to your graphs! We can add a title, xlabel and ylabel with the statements below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.12570541],\n",
       "       [11.08716031],\n",
       "       [ 7.16499527],\n",
       "       [17.73274416],\n",
       "       [21.37317622],\n",
       "       [22.23494769],\n",
       "       [14.74416489],\n",
       "       [ 6.97717328],\n",
       "       [19.5501981 ],\n",
       "       [16.40141772],\n",
       "       [13.17529887],\n",
       "       [17.31843095],\n",
       "       [13.00404942],\n",
       "       [ 7.40253484],\n",
       "       [21.04724983],\n",
       "       [ 7.36938979],\n",
       "       [22.48353561],\n",
       "       [23.11881587],\n",
       "       [22.22389934],\n",
       "       [17.34605184],\n",
       "       [13.59513626],\n",
       "       [14.08678793],\n",
       "       [13.00404942],\n",
       "       [ 7.23680956],\n",
       "       [23.1574851 ],\n",
       "       [19.08064313],\n",
       "       [ 7.88866234],\n",
       "       [12.32457575],\n",
       "       [20.20205088],\n",
       "       [18.72709586],\n",
       "       [ 7.41910737],\n",
       "       [11.25840977],\n",
       "       [22.4172455 ],\n",
       "       [19.00882884],\n",
       "       [ 7.97704916],\n",
       "       [18.83757938],\n",
       "       [16.02577375],\n",
       "       [10.38006576],\n",
       "       [12.25276147],\n",
       "       [18.96463543],\n",
       "       [ 7.4135832 ],\n",
       "       [17.02564962],\n",
       "       [20.20205088],\n",
       "       [14.5563429 ],\n",
       "       [13.88239342],\n",
       "       [15.97053198],\n",
       "       [18.36802442],\n",
       "       [14.46243191],\n",
       "       [19.89269702],\n",
       "       [ 7.6676953 ],\n",
       "       [15.58936383],\n",
       "       [17.49520459],\n",
       "       [16.46770783],\n",
       "       [13.053767  ],\n",
       "       [ 8.97692503],\n",
       "       [12.61183291],\n",
       "       [10.99324931],\n",
       "       [16.1528298 ],\n",
       "       [ 9.60668111],\n",
       "       [16.61133641],\n",
       "       [22.99728399],\n",
       "       [10.75018556],\n",
       "       [17.97580791],\n",
       "       [11.75558562],\n",
       "       [22.94204223],\n",
       "       [10.68389545],\n",
       "       [15.07561546],\n",
       "       [ 7.66217112],\n",
       "       [ 8.32507225],\n",
       "       [20.05289813],\n",
       "       [ 8.45765248],\n",
       "       [17.63883317],\n",
       "       [16.64448147],\n",
       "       [10.58998446],\n",
       "       [18.73262004],\n",
       "       [18.79891015],\n",
       "       [17.92609033],\n",
       "       [11.81635155],\n",
       "       [20.6605575 ],\n",
       "       [20.2904377 ],\n",
       "       [ 8.35269313],\n",
       "       [17.97580791],\n",
       "       [22.05817405],\n",
       "       [12.30800323],\n",
       "       [10.83857238],\n",
       "       [19.53362557],\n",
       "       [18.26306507],\n",
       "       [ 7.34176891],\n",
       "       [14.64472972],\n",
       "       [15.20819568],\n",
       "       [ 8.51841842],\n",
       "       [18.57794311],\n",
       "       [19.22427171],\n",
       "       [21.68253008],\n",
       "       [16.34065178],\n",
       "       [18.89282115],\n",
       "       [11.10925701],\n",
       "       [19.48943216],\n",
       "       [17.85427604],\n",
       "       [ 7.87208981],\n",
       "       [13.62275714],\n",
       "       [11.1589746 ],\n",
       "       [14.40719015],\n",
       "       [19.61648822],\n",
       "       [17.31290678],\n",
       "       [11.1589746 ],\n",
       "       [17.61121229],\n",
       "       [ 9.39676242],\n",
       "       [18.51717717],\n",
       "       [13.41283845],\n",
       "       [14.62815719],\n",
       "       [10.76123392],\n",
       "       [ 9.04873932],\n",
       "       [21.45051468],\n",
       "       [22.73764772],\n",
       "       [20.6826542 ],\n",
       "       [11.36889329],\n",
       "       [ 7.97152498],\n",
       "       [10.63417787],\n",
       "       [12.87146919],\n",
       "       [19.00330467],\n",
       "       [11.0650636 ],\n",
       "       [13.34654833],\n",
       "       [13.73876484],\n",
       "       [21.61071579],\n",
       "       [13.57856373],\n",
       "       [20.15785747],\n",
       "       [22.60506749],\n",
       "       [19.76011679],\n",
       "       [15.21371986],\n",
       "       [17.15270567],\n",
       "       [19.4120937 ],\n",
       "       [20.09709154],\n",
       "       [18.91491785],\n",
       "       [14.35747256],\n",
       "       [10.23091301],\n",
       "       [ 9.89393827],\n",
       "       [ 7.88866234],\n",
       "       [ 8.67861952],\n",
       "       [22.44486638],\n",
       "       [20.18547835],\n",
       "       [14.83255171],\n",
       "       [19.11931237],\n",
       "       [18.36250024],\n",
       "       [20.7986619 ],\n",
       "       [ 8.02124257],\n",
       "       [ 9.03769097],\n",
       "       [ 7.92733157],\n",
       "       [ 8.01019422],\n",
       "       [ 9.40781077],\n",
       "       [ 9.31389978],\n",
       "       [22.64373672],\n",
       "       [11.93235925],\n",
       "       [20.37330034],\n",
       "       [20.05289813],\n",
       "       [19.64963327],\n",
       "       [20.95886301],\n",
       "       [21.58861909],\n",
       "       [17.85427604],\n",
       "       [ 8.31954808]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X @ 𝜃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(x=1)\n",
    "line_X = np.array([[1, 0], [1 ,300]]) # 1 in de eerste kolom is de bias\n",
    "line_y = line_X @ 𝜃\n",
    "plt.plot(line_X[:, 1], line_y, '-');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
