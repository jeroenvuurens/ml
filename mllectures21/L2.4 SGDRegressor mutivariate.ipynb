{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-variate Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ML library uses the same SKLearn libraries under the hood. So the same approach that we used before to include multivariate and polynomial data also works in these libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We use the advertising dataset. One thing to bear in mind is that when we have multiple features which have much different scales, Gradient Descent may have problems converging. Very often it helps to normalize the data to a z-distribution, which we do by supplying scale=True. We will further discuss scaling in the part on data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "from ml import *\n",
    "data = advertising('Sales', scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "# Model\n",
    "\n",
    "Use the SGDRegressor with a 'squared_loss' loss-function and a learning rate alpha of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDRegressor(eta0=1e-2, learning_rate='invscaling', penalty = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss converges quite quickly to around 2.89 which is very close to what we obtained using the normal equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.06753795218285\n",
      "3.0789260599368515\n",
      "2.898927949103308\n",
      "2.897049739085272\n",
      "2.8969786108073494\n",
      "2.8969845495829567\n",
      "2.8969723913170453\n",
      "2.896996546267101\n",
      "2.89697701132697\n",
      "2.8969669401701976\n",
      "2.896967462954102\n"
     ]
    }
   ],
   "source": [
    "for _ in range(101):\n",
    "    model.partial_fit(data.train_X, data.train_y )\n",
    "    if _ % 10 == 0:\n",
    "        y_predict = model.predict(data.train_X)\n",
    "        print(mean_squared_error(y_predict, data.train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding a good learning rate\n",
    "\n",
    "To find a proper learning rate $\\alpha$, try out a few values in the sequence 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, etc. When the learning rate is set too high, the model does not converge. When the learning rate is set too low, it converges very slowly. Usually when you hit the sweet spot you will see that.\n",
    "\n",
    "We will transfer the learning loop into a function to try out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(ùõº):\n",
    "    model = SGDRegressor(eta0=ùõº, learning_rate='invscaling', penalty = None)    \n",
    "    for _ in range(101):\n",
    "        model.partial_fit(data.train_X, data.train_y )\n",
    "        if _ % 10 == 0:\n",
    "            y_predict = model.predict(data.train_X)\n",
    "            print(ùõº, mean_squared_error(y_predict, data.train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "1e-05 258.5584435831246\n",
      "1e-05 257.01064398534857\n",
      "1e-05 255.85755306511993\n",
      "1e-05 254.8440516487964\n",
      "1e-05 253.91415007191193\n",
      "1e-05 253.0429941491671\n",
      "1e-05 252.21664829057016\n",
      "1e-05 251.4262403917478\n",
      "1e-05 250.66562541409704\n",
      "1e-05 249.93043385119714\n",
      "1e-05 249.21732226680652\n",
      "----\n",
      "3e-05 257.9749410400494\n",
      "3e-05 253.36891961783607\n",
      "3e-05 249.97488059461585\n",
      "3e-05 247.0169621786718\n",
      "3e-05 244.32396663921682\n",
      "3e-05 241.8193869119451\n",
      "3e-05 239.4596880897243\n",
      "3e-05 237.21717035553303\n",
      "3e-05 235.07273091577622\n",
      "3e-05 233.01247944727544\n",
      "3e-05 231.02569800695454\n",
      "----\n",
      "0.0001 255.7551312667668\n",
      "0.0001 240.84170134693295\n",
      "0.0001 230.26537739331587\n",
      "0.0001 221.32034592769622\n",
      "0.0001 213.39457652018547\n",
      "0.0001 206.20573827240037\n",
      "0.0001 199.59203127474444\n",
      "0.0001 193.44802007331606\n",
      "0.0001 187.69984089402732\n",
      "0.0001 182.29292751919897\n",
      "0.0001 177.18515961587264\n",
      "----\n",
      "0.0003 249.82173727808703\n",
      "0.0003 208.66367835820247\n",
      "0.0003 182.43375982047615\n",
      "0.0003 162.07803229589655\n",
      "0.0003 145.38242666426677\n",
      "0.0003 131.28975376084682\n",
      "0.0003 119.1747108725655\n",
      "0.0003 108.62681433316489\n",
      "0.0003 99.3553650766951\n",
      "0.0003 91.1447564514671\n",
      "0.0003 83.82965569578214\n",
      "----\n",
      "0.001 230.28782290311915\n",
      "0.001 126.76377787097427\n",
      "0.001 81.56559992455172\n",
      "0.001 55.60729931625485\n",
      "0.001 39.35802292547358\n",
      "0.001 28.684984271140717\n",
      "0.001 21.44655729930476\n",
      "0.001 16.419986769194704\n",
      "0.001 12.865532614791428\n",
      "0.001 10.314951806219904\n",
      "0.001 8.461949192960251\n",
      "----\n",
      "0.003 181.53687690144292\n",
      "0.003 31.769937505429965\n",
      "0.003 10.3165225267351\n",
      "0.003 5.137715924027768\n",
      "0.003 3.641937241847657\n",
      "0.003 3.1621047219934635\n",
      "0.003 2.996249948602094\n",
      "0.003 2.9358526828825586\n",
      "0.003 2.9127546590681606\n",
      "0.003 2.903644298417838\n",
      "0.003 2.8998749978930776\n",
      "----\n",
      "0.01 78.92307514501367\n",
      "0.01 3.0755493733028034\n",
      "0.01 2.8989614326751707\n",
      "0.01 2.8970681327511816\n",
      "0.01 2.8969677412498527\n",
      "0.01 2.897004433529592\n",
      "0.01 2.896974775859724\n",
      "0.01 2.896963187705406\n",
      "0.01 2.896973172068866\n",
      "0.01 2.896959863025901\n",
      "0.01 2.896959970176419\n",
      "----\n",
      "0.03 10.131449043694653\n",
      "0.03 2.8984277200994795\n",
      "0.03 2.897034761916248\n",
      "0.03 2.897333227884156\n",
      "0.03 2.897053560864194\n",
      "0.03 2.8971237758722985\n",
      "0.03 2.8974372385250273\n",
      "0.03 2.8973706148190845\n",
      "0.03 2.8971107526509736\n",
      "0.03 2.8972327350522304\n",
      "0.03 2.8973658925097\n",
      "----\n",
      "0.1 2.9743731546235197\n",
      "0.1 2.9709237469657093\n",
      "0.1 2.9006047424681425\n",
      "0.1 2.9030900610347934\n",
      "0.1 2.9143738978891225\n",
      "0.1 2.918113695992964\n",
      "0.1 2.947142580022485\n",
      "0.1 2.8994735578486286\n",
      "0.1 2.9022710559672227\n",
      "0.1 2.9005371612013575\n",
      "0.1 2.8985672680192915\n",
      "----\n",
      "0.3 3.059586060416931\n",
      "0.3 3.043621049439613\n",
      "0.3 2.945355090170125\n",
      "0.3 3.0210920911543684\n",
      "0.3 2.974549277061282\n",
      "0.3 3.20975715898848\n",
      "0.3 3.050995932961616\n",
      "0.3 3.0510283945494394\n",
      "0.3 2.9733002983150123\n",
      "0.3 3.1933695486969134\n",
      "0.3 3.0784489052456006\n",
      "----\n",
      "1 21.789532233801605\n",
      "1 3.841435291611126\n",
      "1 6.892335165977405\n",
      "1 5.244661518131257\n",
      "1 2.9710306444690184\n",
      "1 3.6938981044828565\n",
      "1 3.018439137984359\n",
      "1 4.290956385107944\n",
      "1 4.374318580784866\n",
      "1 3.34658411065672\n",
      "1 3.488012027797686\n",
      "----\n",
      "3 2.543647107576158e+24\n",
      "3 2.945763025632494e+21\n",
      "3 17.811231474964938\n",
      "3 5.749731591157466\n",
      "3 13.865471581228567\n",
      "3 10.663255945912477\n",
      "3 10.384844095423183\n",
      "3 9.821592969139328\n",
      "3 4.945460315450347\n",
      "3 21.62124516202322\n",
      "3 3.855069043948194\n"
     ]
    }
   ],
   "source": [
    "for ùõº in [1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1, 3]:\n",
    "    print('----')\n",
    "    learn(ùõº)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we observe that setting $\\alpha = 0.01$ on this dataset converges optimally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
