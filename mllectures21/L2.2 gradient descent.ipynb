{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of the Normal Equation\n",
    "\n",
    "We have seen that we can analytically determine the optimum for a linear regression function using the Normal Equation:\n",
    "\n",
    "$$ \\theta = (X^TX)^{-1} \\cdot X^Ty $$\n",
    "\n",
    "Although the Normal Equation is a very fast way to find optimal linear regression functions, it has some limitations in practice:\n",
    "\n",
    "- linear functions\n",
    "- least squares cost function\n",
    "- $X^T \\cdot X$ must be invertible (no colinearity)\n",
    "- Numerical unstable when close to colinear\n",
    "- Computing the inverse has a computational upperbound of $O(n^3)$ where n is the number of features/columns. Therefore the Normal Equation is not efficient for large values of $n$.\n",
    "- When the data set does not fit in RAM, computation becomes difficult and inefficient.\n",
    "\n",
    "There are several alternatives to approximate an optimal function for a given true function. We will focus on Gradient Descent for this purpose, since it is the most general en widely used optimization algorithm.\n",
    "\n",
    "# Gradient Descent\n",
    "\n",
    "According to this method, the coefficients $\\theta$ are adjusted iteratively towards the minimum. To determine the direction for adjusting $\\theta$ we use the partial derivatives of the cost function. An advantage of Gradient Descent is that the approach is applicable to any cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Rule\n",
    "\n",
    "The basis for Gradient Descent is an update rule that adjusts the coefficients towards the minimum. We use a general update rule, in which $\\theta_j$ is a coefficient that is updated, $J(\\theta)$ is the cost function (parameterised by $\\theta$) and $\\alpha$ is the 'learning rate'.\n",
    "\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\frac{\\delta}{\\delta \\theta_j}J(\\theta) $$\n",
    "\n",
    "In the update rule, we can explain the minus-sign by looking at the derivative. Imagine the cost function as being bowl-shaped and our objective is to find the minimum. If the gradient is negative the minimum is to the right, hence we should increase $\\theta$. If the gradient is positive, the minimum is to the left, hence we shoudl decrease $\\theta$.\n",
    "\n",
    "The learning rate $\\alpha$ controls the size of the update steps. We will later discuss the problem of setting the learning rate to a proper value, but for now we will just give a suitable value.\n",
    "\n",
    "We have discussed the cost function $J(\\theta)$ that we can use to analytically determine the optimal $\\theta$. Because we use the derivative of the cost function to find the optimum, it is actually more common to add a factor $\\frac{1}{2m}$. For the optimization problem there is no difference, but this addition shortens the gradient and thus the update rule. \n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^m (\\widehat{y^{(i)}} - y^{(i)})^2 = \\frac{1}{2m} \\sum_{i = 1}^m (\\theta_1 \\cdot x^{(i)} + \\theta_0 - y^{(i)})^2 $$  \n",
    "\n",
    "We now derive the partial derivative by applying the chain-rule:\n",
    "\n",
    "$$ \\frac{\\delta J(\\theta)}{\\delta \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m (\\theta_1 \\cdot x^{(i)} + \\theta_0 - y^{(i)}) \\cdot 1 $$\n",
    "\n",
    "$$ \\frac{\\delta J(\\theta)}{\\delta \\theta_1} = \\frac{1}{m} \\sum_{i=1}^m (\\theta_1 \\cdot x^{(i)} + \\theta_0 - y^{(i)}) \\cdot x^{(i)} $$\n",
    "\n",
    "And the update rules then become:\n",
    "\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m (\\theta_1 \\cdot x^{(i)} + \\theta_0 - y^{(i)}) $$\n",
    "$$ \\theta_1 := \\theta_1 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m (\\theta_1 \\cdot x^{(i)} + \\theta_0 - y^{(i)}) \\cdot x^{(i)} $$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "Although we can compute the updates of all training samples in a loop, this would be very slow in Python. Alternatively, we can rewrite these update rules as dot matrix multiplications, to make the code run much faster. The process of rewriting the agorithm into matrix operations is often callen vectorization.\n",
    "\n",
    "To vectorize our code, we will first rewrite our cost function and update rules using a dot product between $theta$ and $x$. Recall that we have initialize written our hypothesis as $\\theta^T \\cdot x$:\n",
    "\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m (\\theta^T \\cdot x^{(i)} - y^{(i)}) \\cdot x^{(i)}_0 $$\n",
    "\n",
    "$$ \\theta_1 := \\theta_1 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m (\\theta^T \\cdot x^{(i)} - y^{(i)}) \\cdot x^{(i)}_1 $$\n",
    "\n",
    "We can then replace the summation by yet another dot product and take all update rules together in one go:\n",
    "\n",
    "$$ \\theta := \\theta - \\frac{\\alpha}{m} X^T \\cdot (X \\cdot \\theta - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "For starters, we read the Wine data set like before, which gives us an input matrix X and a target vector y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml import *\n",
    "data = advertising_sales_tv(bias=True, column_y = True)\n",
    "X = data.train_X\n",
    "y = data.train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1. ,  93.9],\n",
       "       [  1. ,  75.1],\n",
       "       [  1. ,   4.1],\n",
       "       [  1. , 195.4],\n",
       "       [  1. , 261.3],\n",
       "       [  1. , 276.9],\n",
       "       [  1. , 141.3],\n",
       "       [  1. ,   0.7],\n",
       "       [  1. , 228.3],\n",
       "       [  1. , 171.3],\n",
       "       [  1. , 112.9],\n",
       "       [  1. , 187.9],\n",
       "       [  1. , 109.8],\n",
       "       [  1. ,   8.4],\n",
       "       [  1. , 255.4],\n",
       "       [  1. ,   7.8],\n",
       "       [  1. , 281.4],\n",
       "       [  1. , 292.9],\n",
       "       [  1. , 276.7],\n",
       "       [  1. , 188.4],\n",
       "       [  1. , 120.5],\n",
       "       [  1. , 129.4],\n",
       "       [  1. , 109.8],\n",
       "       [  1. ,   5.4],\n",
       "       [  1. , 293.6],\n",
       "       [  1. , 219.8],\n",
       "       [  1. ,  17.2],\n",
       "       [  1. ,  97.5],\n",
       "       [  1. , 240.1],\n",
       "       [  1. , 213.4],\n",
       "       [  1. ,   8.7],\n",
       "       [  1. ,  78.2],\n",
       "       [  1. , 280.2],\n",
       "       [  1. , 218.5],\n",
       "       [  1. ,  18.8],\n",
       "       [  1. , 215.4],\n",
       "       [  1. , 164.5],\n",
       "       [  1. ,  62.3],\n",
       "       [  1. ,  96.2],\n",
       "       [  1. , 217.7],\n",
       "       [  1. ,   8.6],\n",
       "       [  1. , 182.6],\n",
       "       [  1. , 240.1],\n",
       "       [  1. , 137.9],\n",
       "       [  1. , 125.7],\n",
       "       [  1. , 163.5],\n",
       "       [  1. , 206.9],\n",
       "       [  1. , 136.2],\n",
       "       [  1. , 234.5],\n",
       "       [  1. ,  13.2],\n",
       "       [  1. , 156.6],\n",
       "       [  1. , 191.1],\n",
       "       [  1. , 172.5],\n",
       "       [  1. , 110.7],\n",
       "       [  1. ,  36.9],\n",
       "       [  1. , 102.7],\n",
       "       [  1. ,  73.4],\n",
       "       [  1. , 166.8],\n",
       "       [  1. ,  48.3],\n",
       "       [  1. , 175.1],\n",
       "       [  1. , 290.7],\n",
       "       [  1. ,  69. ],\n",
       "       [  1. , 199.8],\n",
       "       [  1. ,  87.2],\n",
       "       [  1. , 289.7],\n",
       "       [  1. ,  67.8],\n",
       "       [  1. , 147.3],\n",
       "       [  1. ,  13.1],\n",
       "       [  1. ,  25.1],\n",
       "       [  1. , 237.4],\n",
       "       [  1. ,  27.5],\n",
       "       [  1. , 193.7],\n",
       "       [  1. , 175.7],\n",
       "       [  1. ,  66.1],\n",
       "       [  1. , 213.5],\n",
       "       [  1. , 214.7],\n",
       "       [  1. , 198.9],\n",
       "       [  1. ,  88.3],\n",
       "       [  1. , 248.4],\n",
       "       [  1. , 241.7],\n",
       "       [  1. ,  25.6],\n",
       "       [  1. , 199.8],\n",
       "       [  1. , 273.7],\n",
       "       [  1. ,  97.2],\n",
       "       [  1. ,  70.6],\n",
       "       [  1. , 228. ],\n",
       "       [  1. , 205. ],\n",
       "       [  1. ,   7.3],\n",
       "       [  1. , 139.5],\n",
       "       [  1. , 149.7],\n",
       "       [  1. ,  28.6],\n",
       "       [  1. , 210.7],\n",
       "       [  1. , 222.4],\n",
       "       [  1. , 266.9],\n",
       "       [  1. , 170.2],\n",
       "       [  1. , 216.4],\n",
       "       [  1. ,  75.5],\n",
       "       [  1. , 227.2],\n",
       "       [  1. , 197.6],\n",
       "       [  1. ,  16.9],\n",
       "       [  1. , 121. ],\n",
       "       [  1. ,  76.4],\n",
       "       [  1. , 135.2],\n",
       "       [  1. , 229.5],\n",
       "       [  1. , 187.8],\n",
       "       [  1. ,  76.4],\n",
       "       [  1. , 193.2],\n",
       "       [  1. ,  44.5],\n",
       "       [  1. , 209.6],\n",
       "       [  1. , 117.2],\n",
       "       [  1. , 139.2],\n",
       "       [  1. ,  69.2],\n",
       "       [  1. ,  38.2],\n",
       "       [  1. , 262.7],\n",
       "       [  1. , 286. ],\n",
       "       [  1. , 248.8],\n",
       "       [  1. ,  80.2],\n",
       "       [  1. ,  18.7],\n",
       "       [  1. ,  66.9],\n",
       "       [  1. , 107.4],\n",
       "       [  1. , 218.4],\n",
       "       [  1. ,  74.7],\n",
       "       [  1. , 116. ],\n",
       "       [  1. , 123.1],\n",
       "       [  1. , 265.6],\n",
       "       [  1. , 120.2],\n",
       "       [  1. , 239.3],\n",
       "       [  1. , 283.6],\n",
       "       [  1. , 232.1],\n",
       "       [  1. , 149.8],\n",
       "       [  1. , 184.9],\n",
       "       [  1. , 225.8],\n",
       "       [  1. , 238.2],\n",
       "       [  1. , 216.8],\n",
       "       [  1. , 134.3],\n",
       "       [  1. ,  59.6],\n",
       "       [  1. ,  53.5],\n",
       "       [  1. ,  17.2],\n",
       "       [  1. ,  31.5],\n",
       "       [  1. , 280.7],\n",
       "       [  1. , 239.8],\n",
       "       [  1. , 142.9],\n",
       "       [  1. , 220.5],\n",
       "       [  1. , 206.8],\n",
       "       [  1. , 250.9],\n",
       "       [  1. ,  19.6],\n",
       "       [  1. ,  38. ],\n",
       "       [  1. ,  17.9],\n",
       "       [  1. ,  19.4],\n",
       "       [  1. ,  44.7],\n",
       "       [  1. ,  43. ],\n",
       "       [  1. , 284.3],\n",
       "       [  1. ,  90.4],\n",
       "       [  1. , 243.2],\n",
       "       [  1. , 237.4],\n",
       "       [  1. , 230.1],\n",
       "       [  1. , 253.8],\n",
       "       [  1. , 265.2],\n",
       "       [  1. , 197.6],\n",
       "       [  1. ,  25. ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we need is a vector that contains the parameters for our linear function. In Machine Learning, by default we use $\\theta$ (pronounce theta) to hold our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a column vector with 2 zeros: 2 rows, 1 column\n",
    "ùúÉ = np.zeros((2, 1))\n",
    "ùúÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then $\\hat{y} = X \\cdot \\theta$ will give the estimates in a column vector, which we can compare to $y$. In this case, we will get only zeros, because $\\theta_0 = \\theta_1 = 0$, and therefore the dot product with whatever values will be zero. We still need to train the model to find the best values for $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = X @ ùúÉ\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the vectorized update rule we derived above repeatedly and the values of $\\theta$ will converge to the optimal value.\n",
    "\n",
    "We introduce a 'learning rate' to update $\\theta$ with some fraction of the gradient to iteratively take a small step in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "ùõº = 0.00003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00090994]\n",
      " [0.16114593]]\n",
      "[[0.99733529]\n",
      " [0.08482212]]\n",
      "[[1.85090769]\n",
      " [0.08057229]]\n",
      "[[2.58184666]\n",
      " [0.07693303]]\n",
      "[[3.20777106]\n",
      " [0.07381663]]\n",
      "[[3.74376841]\n",
      " [0.07114796]]\n",
      "[[4.20275859]\n",
      " [0.06886271]]\n",
      "[[4.5958053 ]\n",
      " [0.06690578]]\n",
      "[[4.93238267]\n",
      " [0.06523   ]]\n",
      "[[5.22060371]\n",
      " [0.06379498]]\n",
      "[[5.4674158 ]\n",
      " [0.06256613]]\n",
      "[[5.67876821]\n",
      " [0.06151383]]\n",
      "[[5.85975545]\n",
      " [0.06061272]]\n",
      "[[6.01474011]\n",
      " [0.05984107]]\n",
      "[[6.147458  ]\n",
      " [0.05918028]]\n",
      "[[6.26110821]\n",
      " [0.05861443]]\n",
      "[[6.35843021]\n",
      " [0.05812988]]\n",
      "[[6.44176987]\n",
      " [0.05771494]]\n",
      "[[6.51313606]\n",
      " [0.05735962]]\n",
      "[[6.57424902]\n",
      " [0.05705534]]\n",
      "[[6.62658182]\n",
      " [0.05679478]]\n",
      "[[6.67139594]\n",
      " [0.05657166]]\n",
      "[[6.70977157]\n",
      " [0.05638059]]\n",
      "[[6.74263374]\n",
      " [0.05621698]]\n",
      "[[6.77077459]\n",
      " [0.05607687]]\n",
      "[[6.79487241]\n",
      " [0.05595689]]\n",
      "[[6.81550808]\n",
      " [0.05585414]]\n",
      "[[6.83317901]\n",
      " [0.05576616]]\n",
      "[[6.84831114]\n",
      " [0.05569082]]\n",
      "[[6.86126923]\n",
      " [0.0556263 ]]\n",
      "[[6.87236561]\n",
      " [0.05557106]]\n",
      "[[6.88186778]\n",
      " [0.05552375]]\n",
      "[[6.89000475]\n",
      " [0.05548323]]\n",
      "[[6.89697269]\n",
      " [0.05544854]]\n",
      "[[6.90293953]\n",
      " [0.05541883]]\n",
      "[[6.90804911]\n",
      " [0.05539339]]\n",
      "[[6.9124246 ]\n",
      " [0.05537161]]\n",
      "[[6.91617145]\n",
      " [0.05535295]]\n",
      "[[6.91937999]\n",
      " [0.05533698]]\n",
      "[[6.92212756]\n",
      " [0.0553233 ]]\n",
      "[[6.92448039]\n",
      " [0.05531158]]\n",
      "[[6.92649518]\n",
      " [0.05530155]]\n",
      "[[6.9282205 ]\n",
      " [0.05529296]]\n",
      "[[6.92969795]\n",
      " [0.05528561]]\n",
      "[[6.93096313]\n",
      " [0.05527931]]\n",
      "[[6.93204654]\n",
      " [0.05527391]]\n",
      "[[6.93297429]\n",
      " [0.05526929]]\n",
      "[[6.93376876]\n",
      " [0.05526534]]\n",
      "[[6.93444908]\n",
      " [0.05526195]]\n",
      "[[6.93503166]\n",
      " [0.05525905]]\n",
      "[[6.9355305 ]\n",
      " [0.05525657]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(500000):\n",
    "    ùúÉ = ùúÉ - ùõº * X.T @ (X @ ùúÉ - y) / len(X)\n",
    "    if i % 10000 == 0:\n",
    "        print(ùúÉ)\n",
    "print(ùúÉ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the code below, you may notice a few things:\n",
    "\n",
    "- We are getting an estimation of the optimal coefficients instead of the exact outcome\n",
    "- It takes a few seconds to converge, which indicates that it may be less efficient for large data sets. Although there are several tricks we can do to speed up learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
