{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data matrix\n",
    "\n",
    "The first step is to place our training samples in a **Data matrix**. We define a matrix $X$ in which the input samples $(x^{(i)})$ are placed as row vectors $(x^{(i)})^T$:\n",
    "\n",
    "$$ X = \\left[ \\begin{matrix} \n",
    "x_0^{(0)} & x_1^{(0)} \\\\\n",
    "x_0^{(1)} & x_1^{(1)} \\\\\n",
    "\\dots & \\dots \\\\\n",
    "x_0^{(m-1)} & x_1^{(m-1)}\n",
    "\\end{matrix} \\right]\n",
    "$$\n",
    "\n",
    "Let's see how the data is put into a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data, but include a bias parameter\n",
    "data = wines_quality_alcohol(bias=1, column_y = True)\n",
    "X = data.train_X\n",
    "y = data.train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention, we use $y$ (lower case) for a column vector that contains the true values for the target variable, in this case the quality of every bottle of wine in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention, $X$ (upper case) is a matrix in every row contains the values for the input variables of a training sample, in this case a bottle of wine. Here, there are two columns, the second column contains the pH-values for the wines, the first column contains a so-called bias that is always set to 1. By adding this bias column, we can simply use a matrix dot product to estimate the quality of the wines in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a jupyter notebook, the value of the last expression is shown, so we can simply view $X$ and $y$ by entering these in a cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Now that we have prepared our data in $X$ and $y$, we can use the Normal Equation to compute $\\theta$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\theta = (X^TX)^{-1} \\cdot X^TY $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix operations\n",
    "\n",
    "Under the hood, we use the **numpy** library, to store the data in matrices and efficiently perform linear algebra operations on them. Some of the operators are slightly different from what you are used, the `@` operator performs a dot product between matrices and the `.T` transposes a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix inverse\n",
    "\n",
    "In an equation, to the power -1 means the inverse. We won't go into details about matrix inverses, but just show hoow to compute them. \n",
    "\n",
    "One **very important** tip in applying the Normal Equation is to **always use the pseudo-inverse**, because the inverse of a matrix does not always give a 'numerical stable result'. Numpy contains a function to compute the pseudo-inverse: `numpy.linalg.pinv()`. So with that knowledge we can simply compute $\\theta$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import pinv\n",
    "ùúÉ = pinv( X.T @ X ) @ X.T @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps at the beginning of this, you would not have thought learning a model would be this easy, but sometimes applying math does make for a very elegant solution (see: the unreasonable effectiveness of mathematics (Wigner, 1960)).\n",
    "\n",
    "#### So now we have learned $\\theta$, show what the parameters to our regression line $\\widehat{quality} = \\theta_1 \\cdot alcohol + \\theta_0$ are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "Now that we have estimated our model $\\theta$, we can use it to make predictions. For a single data point, we can compute $\\theta^T \\cdot x$ and for multiple data points we can add them to a data matrix $X$ and compute $X \\cdot \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we can plot the data using the `plot()` method. Then we can draw our linear function over it.\n",
    "\n",
    "To plot a regression line, we compute two points. Since the min/max alcohol percentages are resp. 8 and 15, we construct a data matrix with those two points and added a bias=1 to them. Then we can simply multiply the data matrix with $\\theta$ and we get predicted quality for those two points in `line_y`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On the ... fill in the code to add a table with two datapoints. The first column should be a bias column (i.e. always 1) and in the second column place 8 and 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_X = np.array( ... ) \n",
    "line_y = line_X @ ùúÉ\n",
    "line_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use Matplotlib to draw a line over the Scatter plot. In that code, `line_X[:, 1]` is the Numpy way to select only column 1 from all the rows, so leaving out the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(x=1)\n",
    "plt.plot(line_X[:, 1], line_y, '-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
