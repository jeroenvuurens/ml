{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression is not a regression but a classification algorithm. It also happens to be one of the major stepping stones towards Neural Networks and therefore interesting to learn more about. This notebook demonstrates how Logistic Regression works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We can use a classifier on the Wine data set by converting the target variable to a boolean: $y = 1$ if the quality >= 6 or 0 otherwise. The easiest way is to create a new column for our label whether the wine is 'good'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml import *\n",
    "from scipy.special import expit as logit # is more stable in case of overflows\n",
    "data = wines_binary('quality', 'alcohol', 'pH', threshold=6)\n",
    "data.plot2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we plot it so that the plot shows alcohol and pH and then add a bias.\n",
    "data.bias = 1\n",
    "data.column_y = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function and Update Rule\n",
    "\n",
    "To estimate $\\theta$ we use the vectorized version of the cost function:\n",
    "\n",
    "$$ J(\\theta) = -\\frac{1}{m} \\left[ y^T \\cdot log \\left( logit( X \\cdot \\theta ) \\right) + (1 - y^T) \\cdot log \\left( 1 - logit( X \\cdot \\theta ) \\right) \\right] $$\n",
    "\n",
    "and update rule:\n",
    "\n",
    "$$ \\theta := \\theta - \\frac{\\alpha}{m} \\cdot X^T \\cdot (\\ logit(X \\cdot \\theta) - y )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we write a function `fit_model` that uses Batch Gradient Descent to estimate $\\theta$ by repeatedly applying the update rule for some number of iterations. We start by defining the hypothesis $h(X, \\theta)$. Remember that the result of the logit can be interpreted as the likelihood that $y=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(X, ùúÉ):\n",
    "    return logit(X @ ùúÉ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Updates parameters theta for #iterations using the logistic regression update rule\n",
    "X: n x m matrix containing the input for n training examples, each having m features\n",
    "y: n x 1 matrix containing the correct class {0,1} for the n training examples\n",
    "alpha: learning rate\n",
    "iterations: number of iterations\n",
    "returns: theta\n",
    "\"\"\"\n",
    "def fit_model(X, y, alpha=0.00001, iterations=50000):\n",
    "    m = X.shape[1]            # het aantal coefficienten\n",
    "    ùúÉ = np.zeros((m, 1))  # initialiseer theta\n",
    "    for iter in range(iterations):\n",
    "        ùúÉ -= (alpha / m) * X.T @ ( h(X, ùúÉ) - y )\n",
    "    return ùúÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting the coefficients\n",
    "\n",
    "Now, we'll fit the model and look at the values for $\\theta$. Our analysis of the logistic function told us that values of $\\theta^T \\cdot x > 0$ contribute to predicting class 1 and values of $\\theta^T \\cdot x < 0$ contribute to predicting class 1. Therefore, we can see from the sign of $\\theta_1$ that a higher alcohol percentage is associated with good wine and from the sign of $\\theta_2$ that a higher pH-value is associated with bad wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.12640594],\n",
       "       [ 1.0476586 ],\n",
       "       [-2.88445623]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ùúÉ = fit_model(data.train_X, data.train_y)\n",
    "ùúÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction function\n",
    "\n",
    "To use the trained model, we need a `predict` function that classifies a set of cases `X`. Since the outcome of the logistic function can be interpreted as the likelihood that $P(y = 1| x; \\theta)$, we choose to return `True` if our model returns an estimation greater or equal than `0.5` and thus indicates $y=1$, or `False` otherwise indicating $y=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X: n x m matrix containing the input for n training examples, each having m features\n",
    "theta: m x 1 matrix containing the coefficients for the model\n",
    "Returns true if the hypothesis for a given x >= 0.5 otherwise false\n",
    "\"\"\"\n",
    "def predict(X, ùúÉ):\n",
    "    return h(X, ùúÉ) >= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then evaluate our model by comparing the predictions on a set of test cases for which we verify if the prediction equals the True label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X: n x m matrix containing the input for n training examples, each having m features\n",
    "y: n x 1 matrix containing the correct class {0,1} for the n training examples\n",
    "theta: m x 1 matrix containing the coefficients for the model\n",
    "Returns percentage correctly predicted cases in X\n",
    "\"\"\"\n",
    "def evaluate(ùúÉ, X, y):\n",
    "    return sum( predict(X, ùúÉ) == y ) / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has an accuracy of 70.8%, in other words, the percentage of bottles for which correctly predicts whether it is good or bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70758405])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ùúÉ, data.train_X, data.train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding features\n",
    "\n",
    "So what can we do to improve the model's effectiveness? One of the possibilities is to use more features. Unfortunately, we cannot really visualize this in the same way as we did with two features, but trying it out is very easy. In the results below we see that using all features correctly classifies 72.9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = wines_binary('quality', threshold=6, bias=True, column_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72869429])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = fit_model(data.train_X, data.train_y)\n",
    "evaluate(theta, data.train_X, data.train_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
